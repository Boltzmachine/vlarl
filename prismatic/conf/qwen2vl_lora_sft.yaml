### model
# model_name_or_path: Qwen/Qwen2-VL-7B-Instruct
# model_name_or_path: Qwen/Qwen2-VL-2B-Instruct
model_name_or_path: ./MODEL/Qwen2-VL-2B-Instruct
trust_remote_code: true

### method
stage: sft
do_train: true
finetuning_type: lora
lora_target: all

### dataset
dataset_dir: ./data/reward_model_dataset
# dataset: mllm_demo  #,identity,alpaca_en_demo  # video: mllm_video_demo
# dataset: libero_10_no_noops
dataset: libero_spatial_no_noops
# dataset: libero_object_no_noops
template: qwen2_vl
cutoff_len: 2048
max_samples: 1000
overwrite_cache: true
preprocessing_num_workers: 16

### output
# output_dir: ./checkpoints/libero_10_no_noops/prm/adapter
output_dir: ./checkpoints/libero_spatial_no_noops/prm/adapter
# output_dir: ./checkpoints/libero_object_no_noops/prm/adapter
logging_steps: 10
save_steps: 500
plot_loss: true
overwrite_output_dir: true
report_to: wandb
# run_name: debug
# run_name: prm_libero_10
run_name: prm_libero_spatial
# run_name: prm_libero_object
# wandb_project: openvla  # set in env var
# wandb_entity: openvla_cvpr

### train
per_device_train_batch_size: 1
gradient_accumulation_steps: 8
learning_rate: 1.0e-4
# num_train_epochs: 3.0
num_train_epochs: 10.0
lr_scheduler_type: cosine
warmup_ratio: 0.1
bf16: true
ddp_timeout: 180000000

### eval
val_size: 0.1
per_device_eval_batch_size: 1
eval_strategy: steps
eval_steps: 500
